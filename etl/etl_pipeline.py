from etl.extract import extract_data
from etl.transform import transform_data
from etl.load import save_csv, upload_to_s3, insert_to_postgres

def run_pipeline():
    print("ğŸ“¥ Extraction...")
    raw_data = extract_data()

    print("ğŸ” Transformation...")
    df = transform_data(raw_data)
    if df is None or df.empty:
        print("âŒ La transformation a Ã©chouÃ© ou le DataFrame est vide.")
        return

    print("ğŸ’¾ Sauvegarde CSV...")
    csv_path = save_csv(df)

    print("â˜ï¸ Upload S3...")
    upload_to_s3(csv_path, "data-lake-ds-rody", "offres_data_scientist.csv")

    print("ğŸ›¢ï¸ Insertion PostgreSQL...")
    insert_to_postgres(df)

    print("âœ… Pipeline terminÃ© avec succÃ¨s !")
    
    print("ğŸ“Š AperÃ§u des donnÃ©es transformÃ©es :")
    print(df.head())


if __name__ == "__main__":
    run_pipeline()
